{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence = ''\n",
    "with open('./data/uniprot/frequent_kmers.json', 'r') as fp:\n",
    "    kmers = json.load(fp)\n",
    "dictionary = kmers\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "tokenized_data = {}\n",
    "\n",
    "def word_prob(word): \n",
    "    if word in dictionary:\n",
    "        return dictionary[word] / total\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "def words(text): \n",
    "    return re.findall('[A-Z]+', text) \n",
    "\n",
    "\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "print(max_word_length)\n",
    "\n",
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "ctr = 0\n",
    "flag = False\n",
    "for line in tqdm(open('./data/Uniprot/uniprot_sprot.dat')):\n",
    "    ctr += 1\n",
    "#     print(line)\n",
    "    if line.startswith('AC'):\n",
    "        acc_id = line[2:].strip(' ')[:-2]\n",
    "#         print(acc_id, '\\n')\n",
    "#     if ctr % 400 == 0:\n",
    "#         break\n",
    "    if flag:\n",
    "        if line.startswith('/'):\n",
    "            flag = False\n",
    "#             print(sequence, '\\n\\n\\n')\n",
    "            tokenized_data[acc_id] = viterbi_segment(sequence)[0]\n",
    "            sequence = ''\n",
    "            continue\n",
    "        sequence = sequence + ''.join(line[:-1].split(' '))\n",
    "    if line.startswith('SQ'):\n",
    "        flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('./data/Uniprot/sequences/word2grayscale.pkl', 'rb') as pkl:\n",
    "    word2grayscale = pickle.load(pkl)\n",
    "with open('./data/Uniprot/sequences/tokenized_data.pkl', 'rb') as pkl:\n",
    "    tokenized_data = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccb0b64fd5f4f2c8cf8fd14848011e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=563005.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "ctr = 0\n",
    "# !mkdir protein_images\n",
    "DIR = './data/Uniprot/'\n",
    "\n",
    "vec_size=222\n",
    "from PIL import Image\n",
    "for id in tqdm(list(tokenized_data.keys())):\n",
    "    seq = tokenized_data[id]\n",
    "    image = []\n",
    "    for word in seq:\n",
    "        try:\n",
    "            image.append(word2grayscale[word])\n",
    "        except:\n",
    "            image.append(np.zeros(vec_size)) # unknown word\n",
    "    image = np.array(image)\n",
    "    # print(image.shape)\n",
    "#     if vec_size > len(seq):\n",
    "#         padLen = vec_size - len(seq)\n",
    "#         topPad = int(padLen/2)\n",
    "#         bottomPad = math.ceil(padLen/2)\n",
    "#         top = np.zeros((topPad, vec_size))\n",
    "#         bottom = np.zeros((bottomPad, vec_size))\n",
    "#         image = np.vstack((top, image, bottom))\n",
    "#     elif vec_size < len(seq):\n",
    "#         padLen = len(seq) - vec_size\n",
    "#         leftPad = int(padLen/2)\n",
    "#         rightPad = math.ceil(padLen/2)\n",
    "#         left = np.zeros((len(seq), leftPad))\n",
    "#         right = np.zeros((len(seq), rightPad))\n",
    "#         image = np.hstack((left, image, right))\n",
    "    img = Image.fromarray(image.astype(np.uint8))\n",
    "    img = img.resize((128 ,128), Image.ANTIALIAS)\n",
    "    ctr += 1\n",
    "    fid = id.split(';')[0]\n",
    "    try:\n",
    "        if ctr < 56300:\n",
    "            img.save(os.path.join(DIR, 'protein_images/test/proteins/' + fid + '.png'), optimize=True, quality=100)\n",
    "        elif ctr < 2*56300:\n",
    "            img.save(os.path.join(DIR, 'protein_images/valid/proteins/' + fid + '.png'), optimize=True, quality=100)\n",
    "        else:\n",
    "            img.save(os.path.join(DIR, 'protein_images/train/proteins/' + fid + '.png'), optimize=True, quality=100)\n",
    "    except:\n",
    "        print('[SKIP]')\n",
    "        continue\n",
    "#     img.save('./data/Uniprot/protein_images/' + id + '-9.png', optimize=True, quality=100)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb680516267c4732aa0dafd9c0941666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=450403.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_dir = os.path.join(DIR, 'protein_images/train')\n",
    "files = os.listdir(os.path.join(train_dir, 'proteins'))\n",
    "files[:4]\n",
    "import shutil\n",
    "ctr = 1\n",
    "fid = 0\n",
    "try:\n",
    "    os.mkdir(os.path.join(train_dir, str(fid)))\n",
    "    os.mkdir(os.path.join(train_dir, str(fid), 'proteins'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    shutil.move(os.path.join(train_dir, 'proteins', files[i]), os.path.join(train_dir, str(fid), 'proteins', files[i]))\n",
    "    ctr += 1\n",
    "    if ctr % 8192 == 0:\n",
    "        fid += 1\n",
    "        os.mkdir(os.path.join(train_dir, str(fid)))\n",
    "        os.mkdir(os.path.join(train_dir, str(fid), 'proteins'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6329a88f610241a197b8dd3fec0474bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56299.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_dir = os.path.join(DIR, 'protein_images/test')\n",
    "files = os.listdir(os.path.join(train_dir, 'proteins'))\n",
    "files[:4]\n",
    "import shutil\n",
    "ctr = 1\n",
    "fid = 0\n",
    "try:\n",
    "    os.mkdir(os.path.join(train_dir, str(fid)))\n",
    "    os.mkdir(os.path.join(train_dir, str(fid), 'proteins'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    shutil.move(os.path.join(train_dir, 'proteins', files[i]), os.path.join(train_dir, str(fid), 'proteins', files[i]))\n",
    "    ctr += 1\n",
    "    if ctr % 8192 == 0:\n",
    "        fid += 1\n",
    "        os.mkdir(os.path.join(train_dir, str(fid)))\n",
    "        os.mkdir(os.path.join(train_dir, str(fid), 'proteins'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efcefde28fa4471b3875dbb66259108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=56300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_dir = os.path.join(DIR, 'protein_images/valid')\n",
    "files = os.listdir(os.path.join(train_dir, 'proteins'))\n",
    "files[:4]\n",
    "import shutil\n",
    "ctr = 1\n",
    "fid = 0\n",
    "try:\n",
    "    os.mkdir(os.path.join(train_dir, str(fid)))\n",
    "    os.mkdir(os.path.join(train_dir, str(fid), 'proteins'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in tqdm(range(len(files))):\n",
    "    shutil.move(os.path.join(train_dir, 'proteins', files[i]), os.path.join(train_dir, str(fid), 'proteins', files[i]))\n",
    "    ctr += 1\n",
    "    if ctr % 8192 == 0:\n",
    "        fid += 1\n",
    "        os.mkdir(os.path.join(train_dir, str(fid)))\n",
    "        os.mkdir(os.path.join(train_dir, str(fid), 'proteins'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa42d236e645b0ac89048073696994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=55.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_dir = os.path.join(DIR, 'protein_images1/train')\n",
    "dirs = os.listdir(os.path.join(train_dir))\n",
    "dirs[:4]\n",
    "import shutil\n",
    "\n",
    "for j in tqdm(range(len(dirs))):\n",
    "    files = os.listdir(os.path.join(train_dir, dirs[j]))\n",
    "    os.mkdir(os.path.join(train_dir, dirs[j], 'proteins'))\n",
    "    for i in range(len(files)):\n",
    "        shutil.move(os.path.join(train_dir, dirs[j], files[i]), os.path.join(train_dir, dirs[j], 'proteins', files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/Uniprot/sequences/tokenized_data.pkl', 'rb') as pkl:\n",
    "    tokenized_data = pickle.load(pkl)\n",
    "type(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "DIR = './data/uniprot/annotations/'\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "for file in tqdm_notebook(os.listdir(DIR)):\n",
    "    df = pd.read_csv(DIR + file)\n",
    "    df = df[['annotation', 'tag']]\n",
    "    print(f'[CLEANING-{file}]')\n",
    "    df['annotation'] = df['annotation'].apply(cleanText)\n",
    "    print('[SPLITTING]')\n",
    "    train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    print('[TOKENIZING-{train_data}]')\n",
    "    train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['annotation']), tags=[r.tag]), axis=1)\n",
    "    print('[TOKENIZING-{test_data}]')\n",
    "    test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['annotation']), tags=[r.tag]), axis=1)\n",
    "    print(f'[DUMPING]....[SESSION-{file}]')\n",
    "    train_tagged.to_pickle(DIR + 'train1/' + file[:-4] + '.pkl')\n",
    "    test_tagged.to_pickle(DIR + 'test1/' + file[:-4] + '.pkl')\n",
    "    print('[DUMPED]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './data/uniprot/annotations/train/'\n",
    "df = []\n",
    "for file in tqdm_notebook(os.listdir(DIR)):\n",
    "    if len(df) == 0:\n",
    "        print('[INITIALIZING]')\n",
    "        df = pd.read_pickle(DIR + file)\n",
    "        print(f'[INITIALIZED-{file}]')\n",
    "    else:\n",
    "        temp = pd.read_pickle(DIR + file)\n",
    "        print(f'[APPENDING-{file}]')\n",
    "        df = pd.concat([df, temp])\n",
    "        del temp\n",
    "df.to_pickle(DIR + 'train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './data/uniprot/annotations/test/'\n",
    "df = []\n",
    "for file in tqdm_notebook(os.listdir(DIR)):\n",
    "    if len(df) == 0:\n",
    "        print('[INITIALIZING]')\n",
    "        df = pd.read_pickle(DIR + file)\n",
    "        print(f'[INITIALIZED-{file}]')\n",
    "    else:\n",
    "        temp = pd.read_pickle(DIR + file)\n",
    "        print(f'[APPENDING-{file}]')\n",
    "        df = pd.concat([df, temp])\n",
    "        del temp\n",
    "df.to_pickle(DIR + 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "DIR = './data/uniprot/unknown_annotations/'\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "df = []\n",
    "for file in tqdm_notebook(os.listdir(DIR)):\n",
    "    if len(df) == 0:\n",
    "        df = pd.read_csv(DIR + file)\n",
    "    else:\n",
    "        temp = pd.read_csv(DIR + file)\n",
    "        df = pd.concat([df, temp])\n",
    "df = df[['annotation', 'tag']]\n",
    "print(f'[CLEANING-{file}]')\n",
    "df['annotation'] = df['annotation'].apply(cleanText)\n",
    "print('[TOKENIZING-{df}]')\n",
    "tagged = df.apply(lambda r: TaggedDocument(words=tokenize_text(r['annotation']), tags=[r.tag]), axis=1)\n",
    "print(f'[DUMPING]....[SESSION-{file}]')\n",
    "tagged.to_pickle(DIR  + 'unkown.pkl')\n",
    "print('[DUMPED]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/uniprot/sequences/sequences_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "for row in df.iterrows():\n",
    "    print(row[1]['tag'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "DIR = './data/uniprot/sequences/'\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "df = []\n",
    "for file in tqdm_notebook(os.listdir(DIR)):\n",
    "    if file.endswith('.csv'):\n",
    "        if len(df) == 0:\n",
    "            df = pd.read_csv(DIR + file)\n",
    "        else:\n",
    "            temp = pd.read_csv(DIR + file)\n",
    "            df = pd.concat([df, temp])\n",
    "df = df.groupby('tag').filter(lambda x : (x['tag'].count()>=1000).any())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['tag'].nunique(), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['sequence', 'tag']]\n",
    "print(f'[CLEANING-{file}]')\n",
    "df['sequence'] = df['sequence'].apply(cleanText)\n",
    "print('[SPLITTING]')\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print('[TOKENIZING-{train_data}]')\n",
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['sequence']), tags=[r.tag]), axis=1)\n",
    "print('[TOKENIZING-{test_data}]')\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['sequence']), tags=[r.tag]), axis=1)\n",
    "print(f'[DUMPING]....[SESSION-{file}]')\n",
    "train_tagged.to_pickle(DIR + 'new/train/train.pkl')\n",
    "test_tagged.to_pickle(DIR + 'new/test/test.pkl')\n",
    "print('[DUMPED]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "DIR = './data/uniprot/unknown_sequences/'\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\n', r' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "df = []\n",
    "for file in tqdm_notebook(os.listdir(DIR)):\n",
    "    if len(df) == 0:\n",
    "        df = pd.read_csv(DIR + file)\n",
    "    else:\n",
    "        temp = pd.read_csv(DIR + file)\n",
    "        df = pd.concat([df, temp])\n",
    "df = df[['sequence', 'tag']]\n",
    "print(f'[CLEANING-{file}]')\n",
    "df['sequence'] = df['sequence'].apply(cleanText)\n",
    "print('[TOKENIZING-{df}]')\n",
    "tagged = df.apply(lambda r: TaggedDocument(words=tokenize_text(r['sequence']), tags=[r.tag]), axis=1)\n",
    "print(f'[DUMPING]....[SESSION-{file}]')\n",
    "tagged.to_pickle(DIR  + 'unkown.pkl')\n",
    "print('[DUMPED]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
